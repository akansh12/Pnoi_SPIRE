{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "import augment\n",
    "import torchaudio\n",
    "import python_speech_features as psf\n",
    "import random\n",
    "from os import path\n",
    "import glob\n",
    "from scipy.io import wavfile\n",
    "import scipy\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/facebookresearch/WavAugment.git\n",
      "  Cloning https://github.com/facebookresearch/WavAugment.git to /tmp/pip-req-build-iekv9w6d\n",
      "Requirement already satisfied: torch in /home/akansh/anaconda3/lib/python3.7/site-packages (from augment==0.2) (1.7.0)\n",
      "Requirement already satisfied: torchaudio in /home/akansh/anaconda3/lib/python3.7/site-packages (from augment==0.2) (0.7.0)\n",
      "Requirement already satisfied: numpy in /home/akansh/anaconda3/lib/python3.7/site-packages (from torch->augment==0.2) (1.16.2)\n",
      "Requirement already satisfied: dataclasses in /home/akansh/anaconda3/lib/python3.7/site-packages (from torch->augment==0.2) (0.6)\n",
      "Requirement already satisfied: typing-extensions in /home/akansh/anaconda3/lib/python3.7/site-packages (from torch->augment==0.2) (3.7.4.3)\n",
      "Requirement already satisfied: future in /home/akansh/anaconda3/lib/python3.7/site-packages (from torch->augment==0.2) (0.17.1)\n",
      "Building wheels for collected packages: augment\n",
      "  Building wheel for augment (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for augment: filename=augment-0.2-py3-none-any.whl size=5372 sha256=ca497affcfe62875d4242ed900c9445708b2f6eca3787e2822cf1656057f6305\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-e4p5mrko/wheels/de/31/23/bb3c93de3e7e94bda500cba8eff0451435374f4e713d22b0a1\n",
      "Successfully built augment\n",
      "Installing collected packages: augment\n",
      "Successfully installed augment-0.2\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the '/home/akansh/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python -m pip install git+https://github.com/facebookresearch/WavAugment.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python_speech_features\n",
      "  Downloading python_speech_features-0.6.tar.gz (5.6 kB)\n",
      "Building wheels for collected packages: python-speech-features\n",
      "  Building wheel for python-speech-features (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-speech-features: filename=python_speech_features-0.6-py3-none-any.whl size=5889 sha256=955b4ae664dc2969f4d72a4bb295d11f69b1642de54076c8af2b927bbdd3ef8b\n",
      "  Stored in directory: /home/akansh/.cache/pip/wheels/b0/0e/94/28cd6afa3cd5998a63eef99fe31777acd7d758f59cf24839eb\n",
      "Successfully built python-speech-features\n",
      "Installing collected packages: python-speech-features\n",
      "Successfully installed python-speech-features-0.6\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the '/home/akansh/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install python_speech_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_type=['reverb','uniform_noise']\n",
    "loc='/data1/shivani/';\n",
    "data_path=loc+'data_clipped/';\n",
    "dest_loc1=loc+'data_clipped_'+augment_type[0] +'/';\n",
    "dest_loc2=loc+'data_clipped_'+augment_type[1] +'/';\n",
    "\n",
    "if not os.path.isdir(dest_loc1):\n",
    "    os.mkdir(dest_loc1);\n",
    "if not os.path.isdir(dest_loc2):\n",
    "    os.mkdir(dest_loc2);\n",
    "\n",
    "files=glob.glob(data_path+'*.wav');\n",
    "for split_files in np.arange(0,len(files)):\n",
    "    sig, rate=librosa.load(data_loc+j+'/Eko/'+k,sr=None)\n",
    "    x = augment.EffectChain().reverb(50, 50, 50).channels(1).apply(torch.from_numpy(sig), src_info={'rate': rate})\n",
    "    x=x.numpy().squeeze();    \n",
    "    sf.write(dest_loc1+files[split_files][28:],x,rate)\n",
    "\n",
    "    noise_generator = lambda: torch.zeros_like(torch.from_numpy(sig)).uniform_()\n",
    "    snr=random.sample(range(5, 40), 1)\n",
    "    y = augment.EffectChain().additive_noise(noise_generator, snr=int(snr[0])).apply(torch.from_numpy(sig), src_info={'rate': rate})\n",
    "    y=y.numpy().squeeze(); \n",
    "    sf.write(dest_loc2+files[split_files][28:],y,rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 1/7 [00:24<02:26, 24.35s/it]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▊       | 2/7 [00:44<01:55, 23.06s/it]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 3/7 [01:10<01:36, 24.02s/it]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 4/7 [01:37<01:14, 24.86s/it]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████▏  | 5/7 [02:00<00:48, 24.37s/it]\u001b[A\u001b[A\n",
      "\n",
      " 86%|████████▌ | 6/7 [02:29<00:25, 25.68s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 7/7 [02:58<00:00, 26.74s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▎        | 1/8 [00:30<03:33, 30.53s/it]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▌       | 2/8 [00:56<02:54, 29.08s/it]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 3/8 [01:25<02:26, 29.24s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 4/8 [01:48<01:49, 27.30s/it]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▎   | 5/8 [02:16<01:22, 27.36s/it]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▌  | 6/8 [02:46<00:56, 28.19s/it]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 7/8 [03:12<00:27, 27.64s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 8/8 [03:43<00:00, 28.68s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 1/7 [00:27<02:42, 27.15s/it]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▊       | 2/7 [00:55<02:17, 27.52s/it]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 3/7 [01:22<01:49, 27.39s/it]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 4/7 [01:49<01:22, 27.37s/it]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████▏  | 5/7 [02:17<00:55, 27.57s/it]\u001b[A\u001b[A\n",
      "\n",
      " 86%|████████▌ | 6/7 [02:46<00:27, 27.78s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 7/7 [03:10<00:00, 26.77s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▎        | 1/8 [00:31<03:42, 31.78s/it]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▌       | 2/8 [01:04<03:11, 31.95s/it]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 3/8 [01:27<02:27, 29.40s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 4/8 [01:55<01:55, 28.91s/it]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▎   | 5/8 [02:23<01:25, 28.61s/it]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▌  | 6/8 [02:47<00:54, 27.22s/it]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 7/8 [03:14<00:27, 27.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 8/8 [03:40<00:00, 26.76s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▎        | 1/8 [00:27<03:14, 27.82s/it]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▌       | 2/8 [00:57<02:49, 28.25s/it]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 3/8 [01:26<02:23, 28.74s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 4/8 [01:51<01:50, 27.59s/it]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▎   | 5/8 [02:18<01:21, 27.19s/it]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▌  | 6/8 [02:36<00:49, 24.63s/it]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 7/8 [03:05<00:25, 25.80s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 8/8 [03:30<00:00, 25.64s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▎        | 1/8 [00:27<03:09, 27.03s/it]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▌       | 2/8 [00:52<02:39, 26.64s/it]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 3/8 [01:24<02:21, 28.23s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 4/8 [01:52<01:52, 28.16s/it]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▎   | 5/8 [02:19<01:23, 27.72s/it]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▌  | 6/8 [02:47<00:55, 27.77s/it]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 7/8 [03:13<00:27, 27.43s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 8/8 [03:43<00:00, 28.21s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 1/7 [00:27<02:42, 27.07s/it]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▊       | 2/7 [00:55<02:16, 27.38s/it]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 3/7 [01:25<01:52, 28.12s/it]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 4/7 [01:53<01:24, 28.23s/it]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████▏  | 5/7 [02:10<00:49, 24.89s/it]\u001b[A\u001b[A\n",
      "\n",
      " 86%|████████▌ | 6/7 [02:38<00:25, 25.91s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 7/7 [03:06<00:00, 26.53s/it]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "augment_type=['reverb','uniform_noise']\n",
    "set_loc = \"./Sets/\"\n",
    "data_loc = \"./\"\n",
    "for i in os.listdir(set_loc):\n",
    "    set_full = np.loadtxt(set_loc + i, dtype=str)\n",
    "    for j in tqdm(set_full):\n",
    "        os.makedirs('./Augmented_Data/'+'Aug'+j)\n",
    "        os.makedirs('./Augmented_Data/'+'Aug'+j+'/Eko')\n",
    "        os.makedirs('./Augmented_Data/'+'Aug'+j+'/Mic')\n",
    "        for k in os.listdir(data_loc+j+'/Eko'):\n",
    "            sig, rate=librosa.load(data_loc+j+'/Eko/'+k,sr=None)\n",
    "            x = augment.EffectChain().reverb(50, 50, 50).channels(1).apply(torch.from_numpy(sig), src_info={'rate': rate})\n",
    "            x=x.numpy().squeeze();    \n",
    "            sf.write('./Augmented_Data/'+'Aug'+j+'/Eko/reverb'+k,x,rate)\n",
    "            \n",
    "            noise_generator = lambda: torch.zeros_like(torch.from_numpy(sig)).uniform_()\n",
    "            snr=random.sample(range(5, 40), 1)\n",
    "            y = augment.EffectChain().additive_noise(noise_generator, snr=int(snr[0])).apply(torch.from_numpy(sig), src_info={'rate': rate})\n",
    "            y=y.numpy().squeeze(); \n",
    "            sf.write('./Augmented_Data/'+'Aug'+j+'/Eko/uniform_noise'+k,y,rate)\n",
    "            \n",
    "            z = x+y \n",
    "            sf.write('./Augmented_Data/'+'Aug'+j+'/Eko/both'+k,z,rate)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        for k in os.listdir(data_loc+j+'/Mic'):\n",
    "            sig, rate=librosa.load(data_loc+j+'/Mic/'+k,sr=16000)\n",
    "            x = augment.EffectChain().reverb(50, 50, 50).channels(1).apply(torch.from_numpy(sig), src_info={'rate': rate})\n",
    "            x=x.numpy().squeeze();    \n",
    "            sf.write('./Augmented_Data/'+'Aug'+j+'/Mic/reverb'+k,x,rate)\n",
    "\n",
    "            noise_generator = lambda: torch.zeros_like(torch.from_numpy(sig)).uniform_()\n",
    "            snr=random.sample(range(5, 40), 1)\n",
    "            y = augment.EffectChain().additive_noise(noise_generator, snr=int(snr[0])).apply(torch.from_numpy(sig), src_info={'rate': rate})\n",
    "            y=y.numpy().squeeze(); \n",
    "            sf.write('./Augmented_Data/'+'Aug'+j+'/Mic/uniform_noise'+k,y,rate)\n",
    "\n",
    "            z = x+y \n",
    "            sf.write('./Augmented_Data/'+'Aug'+j+'/Mic/both'+k,z,rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig, rate=librosa.load(data_loc+'2019_11_08_Vaishnavi_F_23_153_48_Y_Asthma_post/Mic/Vaishnavi_Mic_2019_11_08_13_25_38.990_B_R_T_DPB_1.wav',sr=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "snr=random.sample(range(5, 40), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12964928"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(581673, 1)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig.reshape(sig.shape[0],1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(data_loc+j+'/'+'Augment_Eko')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./2019_10_11_Bharathi_F_36_162_75_N_HC_NA/Augment_Eko'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loc+j+'/'+'Augment_Eko'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
